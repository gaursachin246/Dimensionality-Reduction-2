{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61e631bf-dfa3-40a3-a2b1-4b21d7fefc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1. What is a projection and how is it used in PCA?\n",
    "\n",
    "\n",
    "\n",
    "#1. Orthogonal transformation: Data is transformed using an orthogonal matrix, consisting of eigenvectors of the covariance matrix.\n",
    "#2. Dimensionality reduction: Data is projected onto a lower-dimensional space, retaining most of the variance.\n",
    "#3. Principal components: The resulting dimensions are called principal components, which are uncorrelated and ordered by explained variance.\n",
    "\n",
    "#The projection process in PCA:\n",
    "\n",
    "#1. Standardize data: Scale data to have zero mean and unit variance.\n",
    "#2. Compute covariance matrix: Calculate the covariance matrix from standardized data.\n",
    "#3. Eigendecomposition: Decompose the covariance matrix into eigenvectors and eigenvalues.\n",
    "#4. Select principal components: Choose the top k eigenvectors corresponding to the largest eigenvalues.\n",
    "#5. Project data: Transform original data onto the selected principal components.\n",
    "\n",
    "#By projecting high-dimensional data onto a lower-dimensional space, PCA:\n",
    "\n",
    "#- Reduces dimensionality\n",
    "#- Retains most of the variance\n",
    "#- Identifies patterns and relationships\n",
    "#- Facilitates data visualization and analysis\n",
    "\n",
    "#The resulting principal components can be used for:\n",
    "\n",
    "#- Feature extraction\n",
    "#- Data compression\n",
    "#- Noise reduction\n",
    "#- Visualization\n",
    "#- Clustering and classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9e87973-02fa-4691-8b55-1a20aac0bec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Q2. How does the optimization problem in PCA work, and what is it trying to achieve?\n",
    "\n",
    "#The goal is to find the orthogonal transformation (projection matrix) that maximizes the variance of the projected data, subject to the constraint that the columns of the projection matrix are orthonormal (orthogonal and normalized).\n",
    "\n",
    "#Mathematical Formulation:\n",
    "\n",
    "#Let X be the high-dimensional data matrix (n x d), where n is the number of samples and d is the number of features.\n",
    "\n",
    "#1. Find the projection matrix W (d x k), where k is the number of principal components.\n",
    "#2. Maximize the variance of the projected data: argmax_W Var(W^T X)\n",
    "#Solution:\n",
    "\n",
    "#The solution to this optimization problem is given by the eigenvectors of the covariance matrix (X^T X) corresponding to the k largest eigenvalues.\n",
    "\n",
    "#1. Compute the covariance matrix: Σ = X^T X\n",
    "#2. Compute the eigendecomposition: Σ = U Λ U^T\n",
    "#3. Select the top k eigenvectors: W = U(:, 1:k)\n",
    "\n",
    "#What is PCA trying to achieve?\n",
    "\n",
    "#PCA is trying to achieve the following:\n",
    "\n",
    "#1. Dimensionality reduction: Reduce the number of features from d to k.\n",
    "#2. Maximize variance: Retain most of the variance in the original data.\n",
    "#3. Identify patterns: Discover hidden patterns and relationships in the data.\n",
    "#4. Noise reduction: Remove noise and irrelevant features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd6275f-449e-4407-b6a2-30dfc3dfd159",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Q3. What is the relationship between covariance matrices and PCA?\n",
    "\n",
    "#Component Analysis) are closely related:\n",
    "\n",
    "#1. Covariance matrix computation: The covariance matrix is computed from the data matrix X, and it represents the variance and covariance between features.\n",
    "\n",
    "#Σ = X^T X / (n - 1)\n",
    "\n",
    "#1. Eigendecomposition: The covariance matrix is decomposed into eigenvectors and eigenvalues using eigendecomposition.\n",
    "\n",
    "#Σ = U Λ U^T\n",
    "\n",
    "#1. Principal components: The eigenvectors (columns of U) are the principal components, which are the directions of maximum variance in the data.\n",
    "2. Eigenvalues: The eigenvalues (diagonal elements of Λ) represent the amount of variance explained by each principal component.\n",
    "3. PCA transformation: The eigenvectors are used to transform the original data into the new coordinate system, where the axes are the principal components.\n",
    "\n",
    "X_pca = X U\n",
    "\n",
    "The covariance matrix is a crucial step in PCA because it:\n",
    "\n",
    "1. Captures feature relationships: Covariance matrix represents the variance and covariance between features.\n",
    "2. Identifies principal components: Eigenvectors of the covariance matrix are the principal components.\n",
    "3. Explains variance: Eigenvalues represent the amount of variance explained by each principal component.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
